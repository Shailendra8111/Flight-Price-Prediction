{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77117cff-160a-4ade-9925-d2e3fdd88978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns',None)\n",
    "data=pd.read_csv(\"Flight_Fare.csv\")\n",
    "data.info()\n",
    "data.describe()\n",
    "\n",
    "import sweetviz as sv # library for univariant analysis\n",
    "my_report=sv.analyze(data) # passing the original dataframe\n",
    "my_report.show_html() # arguments will generate to// the library \n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(data['Airline'],data['Price'])\n",
    "plt.title('Price by Airline')\n",
    "plt.xlabel('Airline')\n",
    "plt.ylabel('Price')\n",
    "plt.xticks(rotation=60)  # Rotate x-axis labels for better readability\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(data['Source'],data['Price'])\n",
    "plt.title('Price-Source')\n",
    "plt.xlabel('Source')\n",
    "plt.ylabel('Price')\n",
    "plt.xticks(rotation=45)  \n",
    "plt.show\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(data['Destination'],data['Price'])\n",
    "plt.title('Price-Destination')\n",
    "plt.xlabel('Destination')\n",
    "plt.ylabel('Price')\n",
    "plt.xticks(rotation=45)  \n",
    "\n",
    "sns.relplot(x='Total_Stops', y='Price', data=data)\n",
    "plt.show()\n",
    "\n",
    "sns.pairplot(data)\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"Flight_Fare.csv\")  \n",
    " \n",
    "# Convert loaded data to DataFrame if it's a list\n",
    "if isinstance(data, list):\n",
    "    data = pd.DataFrame(data)\n",
    "\n",
    "# Function to convert duration to minutes\n",
    "def convert_to_minutes(duration):\n",
    "    parts = duration.split()\n",
    "    hours = 0\n",
    "    minutes = 0\n",
    "    for part in parts:\n",
    "        if 'h' in part:\n",
    "            hours = int(part.strip('h'))\n",
    "        elif 'm' in part:\n",
    "            minutes = int(part.strip('m'))\n",
    "    return hours * 60 + minutes\n",
    "\n",
    "# Apply conversion to 'Duration' column in the dataset\n",
    "data['Duration'] = data['Duration'].apply(convert_to_minutes)\n",
    "\n",
    "# Rename the column to reflect the conversion\n",
    "data.rename(columns={'Duration': 'duration_minutes'}, inplace=True)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(data)\n",
    "\n",
    "# Optionally, save the updated dataset to a new file\n",
    "# data.to_csv('updated_dataset.csv', index=False)  # Replace 'updated_dataset.csv' with the desired file name\n",
    "\n",
    "data.Destination=data.Destination.map({'Cochin':'0','Banglore':'1','Delhi':'2','New Delhi':'3','Hyderabad':'4','Kolkata':'5'})\n",
    "data.head()\n",
    "\n",
    "data.Source=data.Source.map({'Delhi':'0','Kolkata':'1','Banglore':'2','Mumbai':'3','Chennai':'4'})\n",
    "data.head()\n",
    "\n",
    "data.Airline=data.Airline.map({'Jet Airways':'0','IndiGo':'1','Air India':'2','Multiple carriers':'3','SpiceJet':'4','Vistara':'5','Air Asia':'6','GoAir':'7','Multiple carriers Premium economy':'8','Jet Airways Business':'9','Vistara Premium economy':'10','Trujet':'11'})\n",
    "data.head()\n",
    "\n",
    "data.Additional_Info=data.Additional_Info.map({'No info':'0','No Info':'0','In-flight meal not included':'1','No check-in baggage included':'2','1 Long layover':'3','Change airports':'4','Business class':'5','1 Short layover':'6','Red-eye flight':'7','2 Long layover':'8'})\n",
    "data.head()\n",
    "\n",
    "data_encoder=data\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "lc=LabelEncoder()\n",
    "# Total Stops\n",
    "data_encoder.Total_Stops=lc.fit_transform(data_encoder.Total_Stops)\n",
    "# Departure Time\n",
    "data_encoder.Dep_Time=lc.fit_transform(data_encoder.Dep_Time)\n",
    "# Arrival Time \n",
    "data_encoder.Arrival_Time=lc.fit_transform(data_encoder.Arrival_Time)\n",
    "\n",
    "data[['Day', 'Month', 'Year']] = data['Date_of_Journey'].str.split('/', expand=True)\n",
    "\n",
    "# Convert to numeric format if needed\n",
    "data['Day'] = pd.to_numeric(data['Day'])\n",
    "data['Month'] = pd.to_numeric(data['Month'])\n",
    "data['Year'] = pd.to_numeric(data['Year'])\n",
    "\n",
    "data.drop(columns=['Date_of_Journey'], inplace=True)\n",
    "data.drop(columns=['Route'], inplace=True)\n",
    "\n",
    "corr_data=data[['Airline','Source','Destination','Dep_Time','Arrival_Time','duration_minutes','Total_Stops','Day','Month','Price']]\n",
    "\n",
    "X=data.drop('Price',axis=1)\n",
    "y=data.Price\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "scale=MinMaxScaler()\n",
    "\n",
    "X_train=scale.fit_transform(X_train)\n",
    "X_test=scale.fit_transform(X_test)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "# Creating a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Training the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the testing data\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Calculating and printing mean squared error (MSE)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Calculating and printing R-squared\n",
    "r_squared = r2_score(y_test, predictions)\n",
    "print(\"R-squared:\", r_squared)\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "\n",
    "# Creating a Gradient Boosting Regressor\n",
    "gbm = GradientBoostingRegressor()\n",
    "\n",
    "# Training the model on the training data\n",
    "gbm.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the testing data\n",
    "y_pred = gbm.predict(X_test)\n",
    "\n",
    "# Calculating mean squared error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Calculating R-squared\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "# Creating a Gradient Boosting Regressor\n",
    "gbm = GradientBoostingRegressor()\n",
    "\n",
    "# Define the grid of hyperparameters to search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],  # Number of boosting stages to be run\n",
    "    'learning_rate': [0.05, 0.1, 0.2],  # Learning rate shrinks the contribution of each tree\n",
    "    'max_depth': [3, 4, 5]  # Maximum depth of the individual estimators\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=gbm, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Use the best model for predictions\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate mean squared error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Calculate R-squared\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "\n",
    "# Creating a Support Vector Machine (SVM) regression model\n",
    "svm = SVR(kernel='linear')  # You can choose different kernels like 'linear', 'poly', 'rbf', etc.\n",
    "\n",
    "# Training the model on the training data\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the testing data\n",
    "predictions = svm.predict(X_test)\n",
    "\n",
    "# Calculating mean squared error\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Calculating R-squared\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "# Define the grid of hyperparameters to search\n",
    "param_grid = {\n",
    "    'kernel': ['linear', 'rbf'],  # Kernel type\n",
    "    'C': [0.1, 1, 10, 100],        # Regularization parameter\n",
    "    'gamma': ['scale', 'auto']     # Kernel coefficient for 'rbf' kernel\n",
    "}\n",
    "\n",
    "# Creating a Support Vector Machine (SVM) regression model\n",
    "svm = SVR()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Use the best model for predictions\n",
    "best_model = grid_search.best_estimator_\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "# Calculate mean squared error\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Calculate R-squared\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "# Creating a Random Forest Regressor\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)  # You can adjust the number of estimators as needed\n",
    "\n",
    "# Fitting the model on the training data\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the testing data\n",
    "predictions = rf.predict(X_test)\n",
    "\n",
    "# Calculating mean squared error\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Calculating R-squared\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# Encode categorical features\n",
    "data_encoded = pd.get_dummies(data, columns=['Airline', 'Source', 'Destination', 'Additional_Info'])\n",
    "\n",
    "# Splitting the encoded data into training and testing sets\n",
    "X = data_encoded.drop(columns=['Price'])\n",
    "y = data_encoded['Price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creating DMatrix for XGBoost\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Setting XGBoost parameters\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',  # Use regression objective function\n",
    "    'eval_metric': 'rmse'             # Use root mean squared error (RMSE) as evaluation metric\n",
    "}\n",
    "\n",
    "# Training the model\n",
    "num_rounds = 100  # Number of boosting rounds\n",
    "xg_reg = xgb.train(params, dtrain, num_rounds)\n",
    "\n",
    "# Making predictions on the testing data\n",
    "predictions = xg_reg.predict(dtest)\n",
    "\n",
    "# Calculating mean squared error\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Calculating R-squared\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# Creating a Decision Tree Regressor\n",
    "dt = DecisionTreeRegressor(random_state=42)  # You can adjust hyperparameters like max_depth, min_samples_split, etc.\n",
    "\n",
    "# Fitting the model on the training data\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the testing data\n",
    "predictions = dt.predict(X_test)\n",
    "\n",
    "# Calculating mean squared error\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Calculating R-squared\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "# Creating a Decision Tree Regressor\n",
    "dt = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Define the grid of hyperparameters to search\n",
    "param_grid = {\n",
    "    'max_depth': [3, 4, 5, None],             # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5, 10],          # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4]             # Minimum number of samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Use the best model for predictions\n",
    "best_model = grid_search.best_estimator_\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "# Calculate mean squared error\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Calculate R-squared\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(\"R-squared:\", r2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
